GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
Optimizing for cloud
Distributed mixed precision model compiled
Distributed Strategy: MirroredStrategy
Layer: conv2d, dtype: <Policy "mixed_float16">
Layer: batch_normalization, dtype: <Policy "mixed_float16">
Layer: re_lu, dtype: <Policy "mixed_float16">
Layer: conv2d_1, dtype: <Policy "mixed_float16">
Layer: batch_normalization_1, dtype: <Policy "mixed_float16">
Layer: re_lu_1, dtype: <Policy "mixed_float16">
Layer: max_pooling2d, dtype: <Policy "mixed_float16">
Layer: conv2d_2, dtype: <Policy "mixed_float16">
Layer: batch_normalization_2, dtype: <Policy "mixed_float16">
Layer: re_lu_2, dtype: <Policy "mixed_float16">
Layer: conv2d_3, dtype: <Policy "mixed_float16">
Layer: batch_normalization_3, dtype: <Policy "mixed_float16">
Layer: re_lu_3, dtype: <Policy "mixed_float16">
Layer: max_pooling2d_1, dtype: <Policy "mixed_float16">
Layer: conv2d_4, dtype: <Policy "mixed_float16">
Layer: batch_normalization_4, dtype: <Policy "mixed_float16">
Layer: re_lu_4, dtype: <Policy "mixed_float16">
Layer: conv2d_5, dtype: <Policy "mixed_float16">
Layer: batch_normalization_5, dtype: <Policy "mixed_float16">
Layer: re_lu_5, dtype: <Policy "mixed_float16">
Layer: max_pooling2d_2, dtype: <Policy "mixed_float16">
Layer: global_average_pooling2d, dtype: <Policy "mixed_float16">
Layer: dropout, dtype: <Policy "mixed_float16">
Layer: dense, dtype: <Policy "mixed_float16">
Layer: dropout_1, dtype: <Policy "mixed_float16">
Layer: dense_1, dtype: <Policy "mixed_float16">
Training distributed mixed precision model
Time taken to train distributed mixed precision model: 169.03781723976135 seconds

============================================================
üîç STREAMLINED ANALYSIS: part4_deployment_pipeline/distributed_mixed_precision_model.keras
============================================================

üèóÔ∏è  ARCHITECTURE & MEMORY ANALYSIS
--------------------------------------------------
   Total Parameters: 324,394
   Trainable Parameters: 323,498
   Layer conv2d: 896 params, dtype=float32, policy=<Policy "mixed_float16">, size=3.50 KB
   Layer batch_normalization: 128 params, dtype=float32, policy=<Policy "mixed_float16">, size=0.50 KB
   Layer conv2d_1: 9,248 params, dtype=float32, policy=<Policy "mixed_float16">, size=36.12 KB
   Layer batch_normalization_1: 128 params, dtype=float32, policy=<Policy "mixed_float16">, size=0.50 KB
   Layer conv2d_2: 18,496 params, dtype=float32, policy=<Policy "mixed_float16">, size=72.25 KB
   Layer batch_normalization_2: 256 params, dtype=float32, policy=<Policy "mixed_float16">, size=1.00 KB
   Layer conv2d_3: 36,928 params, dtype=float32, policy=<Policy "mixed_float16">, size=144.25 KB
   Layer batch_normalization_3: 256 params, dtype=float32, policy=<Policy "mixed_float16">, size=1.00 KB
   Layer conv2d_4: 73,856 params, dtype=float32, policy=<Policy "mixed_float16">, size=288.50 KB
   Layer batch_normalization_4: 512 params, dtype=float32, policy=<Policy "mixed_float16">, size=2.00 KB
   Layer conv2d_5: 147,584 params, dtype=float32, policy=<Policy "mixed_float16">, size=576.50 KB
   Layer batch_normalization_5: 512 params, dtype=float32, policy=<Policy "mixed_float16">, size=2.00 KB
   Layer dense: 33,024 params, dtype=float32, policy=<Policy "mixed_float16">, size=129.00 KB
   Layer dense_1: 2,570 params, dtype=float32, policy=<Policy "mixed_float16">, size=10.04 KB
   Model File Size: 6.30 MB
   Theoretical Memory: 1.24 MB
   Training Memory: 5.70 MB
   Inference Memory: 1.25 MB
   Memory footprint: 6.95 MB

‚ö° PERFORMANCE & SPEED ANALYSIS
--------------------------------------------------
   Test Accuracy:81.91%
   Test Loss: 0.6123
   Single Sample Time: 11.60 ¬± 119.22 ms
   GPU Available: Yes
   GPU Memory Growth: True
Optimizing for edge
Implementing pruning
Finetuning pruned model
Theoretical size calculation:
  Total parameters: 288698
  After 50% pruning: 144349
  With dynamic range quantization: 0.4130 MB
Applying dynamic range quantization
Saving edge optimized model to: part4_deployment_pipeline/edge_optimized_model.tflite
Measuring model accuracy and latency
Input type: <class 'numpy.float32'>, Quantization: (0.0, 0)
Output type: <class 'numpy.float32'>, Quantization: (0.0, 0)
Model accuracy: 0.7306
Model latency: 1.4610924482345582 ms
Model size: 0.41298580169677734 MB
Optimizing for tinyML
Implementing architecture optimization
Implementing pruning
Finetuning pruned model
Theoretical size calculation:
  Total parameters: 1759686
  After 75% pruning: 439921
  With float16 quantization: 0.8391 MB
Applying float16 quantization
Saving tinyML optimized model to: part4_deployment_pipeline/tinyML_optimized_model.tflite
Measuring model accuracy and latency
Input type: <class 'numpy.float32'>, Quantization: (0.0, 0)
Output type: <class 'numpy.float32'>, Quantization: (0.0, 0)
Model accuracy: 0.432
Model latency: 2.933188772201538 ms
Model size: 0.8390827178955078 MB
For use case cloud_server, deploy with ['mixed_precision', 'distributed'] to achieve the best performance
For use case edge_device, deploy with ['0.5_pruning', 'dynamic_range_quantization'] to achieve the best performance
For use case microcontroller, deploy with ['architecture_optimization', '0.75_pruning', 'float16_quantization'] to achieve the best performance
The targets that meet the performance requirements are: {'cloud_server': 'cloud_server', 'edge_device': 'edge_device', 'microcontroller': 'microcontroller'}
Multi-Scale Optimization Complete!
Report saved to: multi_scale_optimization_report.json
