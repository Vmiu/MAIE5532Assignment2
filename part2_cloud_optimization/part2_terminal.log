GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
Implementing mixed precision
Training time: 116.21325588226318 seconds

============================================================
üîç STREAMLINED ANALYSIS: part2_cloud_optimization/mixed_precision_model.keras
============================================================

üèóÔ∏è  ARCHITECTURE & MEMORY ANALYSIS
--------------------------------------------------
   Total Parameters: 324,394
   Trainable Parameters: 323,498
   Layer conv2d: 896 params, dtype=float32, policy=<Policy "mixed_float16">, size=3.50 KB
   Layer batch_normalization: 128 params, dtype=float32, policy=<Policy "mixed_float16">, size=0.50 KB
   Layer conv2d_1: 9,248 params, dtype=float32, policy=<Policy "mixed_float16">, size=36.12 KB
   Layer batch_normalization_1: 128 params, dtype=float32, policy=<Policy "mixed_float16">, size=0.50 KB
   Layer conv2d_2: 18,496 params, dtype=float32, policy=<Policy "mixed_float16">, size=72.25 KB
   Layer batch_normalization_2: 256 params, dtype=float32, policy=<Policy "mixed_float16">, size=1.00 KB
   Layer conv2d_3: 36,928 params, dtype=float32, policy=<Policy "mixed_float16">, size=144.25 KB
   Layer batch_normalization_3: 256 params, dtype=float32, policy=<Policy "mixed_float16">, size=1.00 KB
   Layer conv2d_4: 73,856 params, dtype=float32, policy=<Policy "mixed_float16">, size=288.50 KB
   Layer batch_normalization_4: 512 params, dtype=float32, policy=<Policy "mixed_float16">, size=2.00 KB
   Layer conv2d_5: 147,584 params, dtype=float32, policy=<Policy "mixed_float16">, size=576.50 KB
   Layer batch_normalization_5: 512 params, dtype=float32, policy=<Policy "mixed_float16">, size=2.00 KB
   Layer dense: 33,024 params, dtype=float32, policy=<Policy "mixed_float16">, size=129.00 KB
   Layer dense_1: 2,570 params, dtype=float32, policy=<Policy "mixed_float16">, size=10.04 KB
   Model File Size: 6.30 MB
   Theoretical Memory: 1.24 MB
   Training Memory: 5.70 MB
   Inference Memory: 1.25 MB
   Memory footprint: 6.95 MB

‚ö° PERFORMANCE & SPEED ANALYSIS
--------------------------------------------------
   Test Accuracy:82.50%
   Test Loss: 0.5324
   Single Sample Time: 3.22 ¬± 22.66 ms
   GPU Available: Yes
   GPU Memory Growth: True
Distributed model compiled with OneDeviceStrategy
Distributed model compiled with MirroredStrategy
OneDevice training time: 139.07677364349365 seconds
Distributed training time: 174.65950965881348 seconds
scaling efficiency: 0.7962736979805537
Starting gradient accumulation batch processing benchmark...
=== GRADIENT ACCUMULATION THROUGHPUT ANALYSIS ===
Testing throughput for batch size: 64
  Accumulation steps: 1
  Results: 2613.29 samples/sec, 40.83 batches/sec
Testing throughput for batch size: 128
  Accumulation steps: 2
  Results: 2338.89 samples/sec, 18.27 batches/sec
Testing throughput for batch size: 256
  Accumulation steps: 4
  Results: 2837.72 samples/sec, 11.08 batches/sec
Testing throughput for batch size: 512
  Accumulation steps: 8
  Results: 3738.11 samples/sec, 7.30 batches/sec
Testing throughput for batch size: 1024
  Accumulation steps: 16
  Results: 4186.59 samples/sec, 4.09 batches/sec
Optimal batch size for throughput: 1024
Best throughput: 4186.59 samples/sec
Testing throughput for batch size: 1024
  Accumulation steps: 16
  Results: 4006.68 samples/sec, 3.91 batches/sec
Starting knowledge distillation training...
Batch 0/781 - Loss: 7.3678 - Acc: 0.1562
Batch 100/781 - Loss: 6.8022 - Acc: 0.2257
Batch 200/781 - Loss: 6.7630 - Acc: 0.2581
Batch 300/781 - Loss: 6.7139 - Acc: 0.2850
Batch 400/781 - Loss: 6.6641 - Acc: 0.3117
Batch 500/781 - Loss: 6.6241 - Acc: 0.3326
Batch 600/781 - Loss: 6.5720 - Acc: 0.3512
Batch 700/781 - Loss: 6.5407 - Acc: 0.3674
Epoch 1 - Train Loss: 6.5285 - Train Acc: 0.3798 - Val Loss: 7.7556 - Val Acc: 0.3360
Batch 0/781 - Loss: 6.5589 - Acc: 0.4531
Batch 100/781 - Loss: 6.2367 - Acc: 0.4901
Batch 200/781 - Loss: 6.2581 - Acc: 0.4938
Batch 300/781 - Loss: 6.2538 - Acc: 0.5022
Batch 400/781 - Loss: 6.2439 - Acc: 0.5089
Batch 500/781 - Loss: 6.2355 - Acc: 0.5153
Batch 600/781 - Loss: 6.2162 - Acc: 0.5166
Batch 700/781 - Loss: 6.2072 - Acc: 0.5215
Epoch 2 - Train Loss: 6.2104 - Train Acc: 0.5259 - Val Loss: 7.3470 - Val Acc: 0.4582
Batch 0/781 - Loss: 6.4583 - Acc: 0.6094
Batch 100/781 - Loss: 6.0843 - Acc: 0.5657
Batch 200/781 - Loss: 6.1115 - Acc: 0.5657
Batch 300/781 - Loss: 6.1115 - Acc: 0.5673
Batch 400/781 - Loss: 6.1120 - Acc: 0.5682
Batch 500/781 - Loss: 6.1082 - Acc: 0.5715
Batch 600/781 - Loss: 6.0932 - Acc: 0.5712
Batch 700/781 - Loss: 6.0891 - Acc: 0.5745
Epoch 3 - Train Loss: 6.0957 - Train Acc: 0.5774 - Val Loss: 6.9218 - Val Acc: 0.5975
Batch 0/781 - Loss: 6.3150 - Acc: 0.5781
Batch 100/781 - Loss: 5.9963 - Acc: 0.5995
Batch 200/781 - Loss: 6.0278 - Acc: 0.5973
Batch 300/781 - Loss: 6.0315 - Acc: 0.5989
Batch 400/781 - Loss: 6.0345 - Acc: 0.6008
Batch 500/781 - Loss: 6.0313 - Acc: 0.6044
Batch 600/781 - Loss: 6.0176 - Acc: 0.6050
Batch 700/781 - Loss: 6.0154 - Acc: 0.6071
Epoch 4 - Train Loss: 6.0230 - Train Acc: 0.6097 - Val Loss: 7.1939 - Val Acc: 0.5098
Batch 0/781 - Loss: 6.2775 - Acc: 0.5469
Batch 100/781 - Loss: 5.9407 - Acc: 0.6202
Batch 200/781 - Loss: 5.9727 - Acc: 0.6226
Batch 300/781 - Loss: 5.9709 - Acc: 0.6265
Batch 400/781 - Loss: 5.9713 - Acc: 0.6294
Batch 500/781 - Loss: 5.9707 - Acc: 0.6313
Batch 600/781 - Loss: 5.9581 - Acc: 0.6320
Batch 700/781 - Loss: 5.9580 - Acc: 0.6322
Epoch 5 - Train Loss: 5.9667 - Train Acc: 0.6349 - Val Loss: 6.9463 - Val Acc: 0.5974
Batch 0/781 - Loss: 6.1611 - Acc: 0.6094
Batch 100/781 - Loss: 5.8882 - Acc: 0.6487
Batch 200/781 - Loss: 5.9221 - Acc: 0.6496
Batch 300/781 - Loss: 5.9261 - Acc: 0.6504
Batch 400/781 - Loss: 5.9277 - Acc: 0.6504
Batch 500/781 - Loss: 5.9261 - Acc: 0.6529
Batch 600/781 - Loss: 5.9147 - Acc: 0.6533
Batch 700/781 - Loss: 5.9150 - Acc: 0.6538
Epoch 6 - Train Loss: 5.9221 - Train Acc: 0.6572 - Val Loss: 6.8783 - Val Acc: 0.6187
Batch 0/781 - Loss: 6.1248 - Acc: 0.6406
Batch 100/781 - Loss: 5.8454 - Acc: 0.6700
Batch 200/781 - Loss: 5.8826 - Acc: 0.6695
Batch 300/781 - Loss: 5.8861 - Acc: 0.6710
Batch 400/781 - Loss: 5.8914 - Acc: 0.6700
Batch 500/781 - Loss: 5.8902 - Acc: 0.6721
Batch 600/781 - Loss: 5.8780 - Acc: 0.6723
Batch 700/781 - Loss: 5.8786 - Acc: 0.6731
Epoch 7 - Train Loss: 5.8887 - Train Acc: 0.6746 - Val Loss: 6.7742 - Val Acc: 0.6570
Batch 0/781 - Loss: 6.0864 - Acc: 0.6406
Batch 100/781 - Loss: 5.8109 - Acc: 0.6838
Batch 200/781 - Loss: 5.8452 - Acc: 0.6835
Batch 300/781 - Loss: 5.8508 - Acc: 0.6850
Batch 400/781 - Loss: 5.8561 - Acc: 0.6841
Batch 500/781 - Loss: 5.8560 - Acc: 0.6872
Batch 600/781 - Loss: 5.8469 - Acc: 0.6856
Batch 700/781 - Loss: 5.8481 - Acc: 0.6860
Epoch 8 - Train Loss: 5.8563 - Train Acc: 0.6885 - Val Loss: 6.8997 - Val Acc: 0.6210
Batch 0/781 - Loss: 5.9668 - Acc: 0.7188
Batch 100/781 - Loss: 5.7976 - Acc: 0.6920
Batch 200/781 - Loss: 5.8294 - Acc: 0.6926
Batch 300/781 - Loss: 5.8282 - Acc: 0.6964
Batch 400/781 - Loss: 5.8363 - Acc: 0.6956
Batch 500/781 - Loss: 5.8359 - Acc: 0.6985
Batch 600/781 - Loss: 5.8231 - Acc: 0.6993
Batch 700/781 - Loss: 5.8231 - Acc: 0.7012
Epoch 9 - Train Loss: 5.8321 - Train Acc: 0.7026 - Val Loss: 6.9260 - Val Acc: 0.6232
Batch 0/781 - Loss: 5.9816 - Acc: 0.6719
Batch 100/781 - Loss: 5.7716 - Acc: 0.7050
Batch 200/781 - Loss: 5.8062 - Acc: 0.7039
Batch 300/781 - Loss: 5.8056 - Acc: 0.7079
Batch 400/781 - Loss: 5.8112 - Acc: 0.7084
Batch 500/781 - Loss: 5.8110 - Acc: 0.7105
Batch 600/781 - Loss: 5.7989 - Acc: 0.7097
Batch 700/781 - Loss: 5.8002 - Acc: 0.7103
Epoch 10 - Train Loss: 5.8095 - Train Acc: 0.7118 - Val Loss: 6.7328 - Val Acc: 0.6796
Batch 0/781 - Loss: 6.0023 - Acc: 0.6719
Batch 100/781 - Loss: 5.7444 - Acc: 0.7195
Batch 200/781 - Loss: 5.7763 - Acc: 0.7213
Batch 300/781 - Loss: 5.7775 - Acc: 0.7221
Batch 400/781 - Loss: 5.7857 - Acc: 0.7216
Batch 500/781 - Loss: 5.7868 - Acc: 0.7233
Batch 600/781 - Loss: 5.7761 - Acc: 0.7226
Batch 700/781 - Loss: 5.7766 - Acc: 0.7240
Epoch 11 - Train Loss: 5.7866 - Train Acc: 0.7260 - Val Loss: 6.7180 - Val Acc: 0.6910
Batch 0/781 - Loss: 5.9482 - Acc: 0.7188
Batch 100/781 - Loss: 5.7257 - Acc: 0.7269
Batch 200/781 - Loss: 5.7564 - Acc: 0.7311
Batch 300/781 - Loss: 5.7597 - Acc: 0.7325
Batch 400/781 - Loss: 5.7682 - Acc: 0.7309
Batch 500/781 - Loss: 5.7678 - Acc: 0.7331
Batch 600/781 - Loss: 5.7569 - Acc: 0.7328
Batch 700/781 - Loss: 5.7595 - Acc: 0.7330
Epoch 12 - Train Loss: 5.7695 - Train Acc: 0.7347 - Val Loss: 6.7261 - Val Acc: 0.6967
Batch 0/781 - Loss: 5.9935 - Acc: 0.7188
Batch 100/781 - Loss: 5.7059 - Acc: 0.7468
Batch 200/781 - Loss: 5.7418 - Acc: 0.7420
Batch 300/781 - Loss: 5.7467 - Acc: 0.7399
Batch 400/781 - Loss: 5.7550 - Acc: 0.7382
Batch 500/781 - Loss: 5.7556 - Acc: 0.7405
Batch 600/781 - Loss: 5.7440 - Acc: 0.7397
Batch 700/781 - Loss: 5.7442 - Acc: 0.7407
Epoch 13 - Train Loss: 5.7542 - Train Acc: 0.7422 - Val Loss: 6.6526 - Val Acc: 0.7149
Batch 0/781 - Loss: 5.9418 - Acc: 0.7031
Batch 100/781 - Loss: 5.6904 - Acc: 0.7495
Batch 200/781 - Loss: 5.7269 - Acc: 0.7484
Batch 300/781 - Loss: 5.7285 - Acc: 0.7492
Batch 400/781 - Loss: 5.7381 - Acc: 0.7469
Batch 500/781 - Loss: 5.7413 - Acc: 0.7473
Batch 600/781 - Loss: 5.7296 - Acc: 0.7469
Batch 700/781 - Loss: 5.7306 - Acc: 0.7480
Epoch 14 - Train Loss: 5.7409 - Train Acc: 0.7496 - Val Loss: 6.7199 - Val Acc: 0.6909
Batch 0/781 - Loss: 5.8527 - Acc: 0.8125
Batch 100/781 - Loss: 5.6775 - Acc: 0.7531
Batch 200/781 - Loss: 5.7143 - Acc: 0.7532
Batch 300/781 - Loss: 5.7183 - Acc: 0.7536
Batch 400/781 - Loss: 5.7272 - Acc: 0.7516
Batch 500/781 - Loss: 5.7288 - Acc: 0.7534
Batch 600/781 - Loss: 5.7187 - Acc: 0.7519
Batch 700/781 - Loss: 5.7200 - Acc: 0.7524
Epoch 15 - Train Loss: 5.7306 - Train Acc: 0.7537 - Val Loss: 6.7856 - Val Acc: 0.6710
Batch 0/781 - Loss: 5.8442 - Acc: 0.8594
Batch 100/781 - Loss: 5.6604 - Acc: 0.7718
Batch 200/781 - Loss: 5.7067 - Acc: 0.7620
Batch 300/781 - Loss: 5.7114 - Acc: 0.7599
Batch 400/781 - Loss: 5.7185 - Acc: 0.7584
Batch 500/781 - Loss: 5.7177 - Acc: 0.7614
Batch 600/781 - Loss: 5.7067 - Acc: 0.7611
Batch 700/781 - Loss: 5.7078 - Acc: 0.7614
Epoch 16 - Train Loss: 5.7183 - Train Acc: 0.7629 - Val Loss: 6.6311 - Val Acc: 0.7271
Batch 0/781 - Loss: 5.8822 - Acc: 0.8125
Batch 100/781 - Loss: 5.6511 - Acc: 0.7676
Batch 200/781 - Loss: 5.6961 - Acc: 0.7627
Batch 300/781 - Loss: 5.7005 - Acc: 0.7627
Batch 400/781 - Loss: 5.7078 - Acc: 0.7626
Batch 500/781 - Loss: 5.7101 - Acc: 0.7643
Batch 600/781 - Loss: 5.6985 - Acc: 0.7636
Batch 700/781 - Loss: 5.6991 - Acc: 0.7650
Epoch 17 - Train Loss: 5.7083 - Train Acc: 0.7674 - Val Loss: 6.7477 - Val Acc: 0.6878
Batch 0/781 - Loss: 5.9044 - Acc: 0.7500
Batch 100/781 - Loss: 5.6446 - Acc: 0.7712
Batch 200/781 - Loss: 5.6833 - Acc: 0.7680
Batch 300/781 - Loss: 5.6906 - Acc: 0.7661
Batch 400/781 - Loss: 5.6985 - Acc: 0.7657
Batch 500/781 - Loss: 5.7002 - Acc: 0.7678
Batch 600/781 - Loss: 5.6895 - Acc: 0.7674
Batch 700/781 - Loss: 5.6908 - Acc: 0.7687
Epoch 18 - Train Loss: 5.7016 - Train Acc: 0.7700 - Val Loss: 6.6273 - Val Acc: 0.7332
Batch 0/781 - Loss: 5.9200 - Acc: 0.7812
Batch 100/781 - Loss: 5.6350 - Acc: 0.7826
Batch 200/781 - Loss: 5.6781 - Acc: 0.7749
Batch 300/781 - Loss: 5.6809 - Acc: 0.7744
Batch 400/781 - Loss: 5.6886 - Acc: 0.7739
Batch 500/781 - Loss: 5.6885 - Acc: 0.7764
Batch 600/781 - Loss: 5.6777 - Acc: 0.7748
Batch 700/781 - Loss: 5.6796 - Acc: 0.7757
Epoch 19 - Train Loss: 5.6911 - Train Acc: 0.7768 - Val Loss: 6.5831 - Val Acc: 0.7456
Batch 0/781 - Loss: 5.8497 - Acc: 0.8125
Batch 100/781 - Loss: 5.6309 - Acc: 0.7825
Batch 200/781 - Loss: 5.6697 - Acc: 0.7779
Batch 300/781 - Loss: 5.6710 - Acc: 0.7799
Batch 400/781 - Loss: 5.6795 - Acc: 0.7797
Batch 500/781 - Loss: 5.6818 - Acc: 0.7812
Batch 600/781 - Loss: 5.6706 - Acc: 0.7802
Batch 700/781 - Loss: 5.6721 - Acc: 0.7813
Epoch 20 - Train Loss: 5.6833 - Train Acc: 0.7824 - Val Loss: 6.6050 - Val Acc: 0.7386
Knowledge distillation training completed!

============================================================
üîç STREAMLINED ANALYSIS: part2_cloud_optimization/student_model.keras
============================================================

üèóÔ∏è  ARCHITECTURE & MEMORY ANALYSIS
--------------------------------------------------
   Total Parameters: 82,586
   Trainable Parameters: 82,138
   Layer conv2d: 448 params, dtype=float32, policy=<Policy "float32">, size=1.75 KB
   Layer batch_normalization: 64 params, dtype=float32, policy=<Policy "float32">, size=0.25 KB
   Layer conv2d_1: 2,320 params, dtype=float32, policy=<Policy "float32">, size=9.06 KB
   Layer batch_normalization_1: 64 params, dtype=float32, policy=<Policy "float32">, size=0.25 KB
   Layer conv2d_2: 4,640 params, dtype=float32, policy=<Policy "float32">, size=18.12 KB
   Layer batch_normalization_2: 128 params, dtype=float32, policy=<Policy "float32">, size=0.50 KB
   Layer conv2d_3: 9,248 params, dtype=float32, policy=<Policy "float32">, size=36.12 KB
   Layer batch_normalization_3: 128 params, dtype=float32, policy=<Policy "float32">, size=0.50 KB
   Layer conv2d_4: 18,496 params, dtype=float32, policy=<Policy "float32">, size=72.25 KB
   Layer batch_normalization_4: 256 params, dtype=float32, policy=<Policy "float32">, size=1.00 KB
   Layer conv2d_5: 36,928 params, dtype=float32, policy=<Policy "float32">, size=144.25 KB
   Layer batch_normalization_5: 256 params, dtype=float32, policy=<Policy "float32">, size=1.00 KB
   Layer dense: 8,320 params, dtype=float32, policy=<Policy "float32">, size=32.50 KB
   Layer dense_1: 1,290 params, dtype=float32, policy=<Policy "float32">, size=5.04 KB
   Model File Size: 0.39 MB
   Theoretical Memory: 0.32 MB
   Training Memory: 2.01 MB
   Inference Memory: 0.33 MB
   Memory footprint: 2.34 MB

‚ö° PERFORMANCE & SPEED ANALYSIS
--------------------------------------------------
   Test Accuracy:73.87%
   Test Loss: 0.9342
   Single Sample Time: 2.37 ¬± 19.09 ms
   GPU Available: Yes
   GPU Memory Growth: True

============================================================
üîç STREAMLINED ANALYSIS: part2_cloud_optimization/teacher_model.keras
============================================================

üèóÔ∏è  ARCHITECTURE & MEMORY ANALYSIS
--------------------------------------------------
   Total Parameters: 324,394
   Trainable Parameters: 323,498
   Layer conv2d: 896 params, dtype=float32, policy=<Policy "float32">, size=3.50 KB
   Layer batch_normalization: 128 params, dtype=float32, policy=<Policy "float32">, size=0.50 KB
   Layer conv2d_1: 9,248 params, dtype=float32, policy=<Policy "float32">, size=36.12 KB
   Layer batch_normalization_1: 128 params, dtype=float32, policy=<Policy "float32">, size=0.50 KB
   Layer conv2d_2: 18,496 params, dtype=float32, policy=<Policy "float32">, size=72.25 KB
   Layer batch_normalization_2: 256 params, dtype=float32, policy=<Policy "float32">, size=1.00 KB
   Layer conv2d_3: 36,928 params, dtype=float32, policy=<Policy "float32">, size=144.25 KB
   Layer batch_normalization_3: 256 params, dtype=float32, policy=<Policy "float32">, size=1.00 KB
   Layer conv2d_4: 73,856 params, dtype=float32, policy=<Policy "float32">, size=288.50 KB
   Layer batch_normalization_4: 512 params, dtype=float32, policy=<Policy "float32">, size=2.00 KB
   Layer conv2d_5: 147,584 params, dtype=float32, policy=<Policy "float32">, size=576.50 KB
   Layer batch_normalization_5: 512 params, dtype=float32, policy=<Policy "float32">, size=2.00 KB
   Layer dense: 33,024 params, dtype=float32, policy=<Policy "float32">, size=129.00 KB
   Layer dense_1: 2,570 params, dtype=float32, policy=<Policy "float32">, size=10.04 KB
   Model File Size: 1.32 MB
   Theoretical Memory: 1.24 MB
   Training Memory: 5.70 MB
   Inference Memory: 1.25 MB
   Memory footprint: 6.95 MB

‚ö° PERFORMANCE & SPEED ANALYSIS
--------------------------------------------------
   Test Accuracy:82.31%
   Test Loss: 1.2696
   Single Sample Time: 1.58 ¬± 2.86 ms
   GPU Available: Yes
   GPU Memory Growth: True
