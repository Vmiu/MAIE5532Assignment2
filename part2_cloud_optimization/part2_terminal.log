GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
Implementing mixed precision
Training time: 107.75819540023804 seconds

============================================================
üîç STREAMLINED ANALYSIS: part2_cloud_optimization/mixed_precision_model.keras
============================================================

üèóÔ∏è  ARCHITECTURE & MEMORY ANALYSIS
--------------------------------------------------
   Total Parameters: 324,394
   Trainable Parameters: 323,498
   Layer conv2d: 896 params, dtype=float32, policy=<Policy "mixed_float16">, size=3.50 KB
   Layer batch_normalization: 128 params, dtype=float32, policy=<Policy "mixed_float16">, size=0.50 KB
   Layer conv2d_1: 9,248 params, dtype=float32, policy=<Policy "mixed_float16">, size=36.12 KB
   Layer batch_normalization_1: 128 params, dtype=float32, policy=<Policy "mixed_float16">, size=0.50 KB
   Layer conv2d_2: 18,496 params, dtype=float32, policy=<Policy "mixed_float16">, size=72.25 KB
   Layer batch_normalization_2: 256 params, dtype=float32, policy=<Policy "mixed_float16">, size=1.00 KB
   Layer conv2d_3: 36,928 params, dtype=float32, policy=<Policy "mixed_float16">, size=144.25 KB
   Layer batch_normalization_3: 256 params, dtype=float32, policy=<Policy "mixed_float16">, size=1.00 KB
   Layer conv2d_4: 73,856 params, dtype=float32, policy=<Policy "mixed_float16">, size=288.50 KB
   Layer batch_normalization_4: 512 params, dtype=float32, policy=<Policy "mixed_float16">, size=2.00 KB
   Layer conv2d_5: 147,584 params, dtype=float32, policy=<Policy "mixed_float16">, size=576.50 KB
   Layer batch_normalization_5: 512 params, dtype=float32, policy=<Policy "mixed_float16">, size=2.00 KB
   Layer dense: 33,024 params, dtype=float32, policy=<Policy "mixed_float16">, size=129.00 KB
   Layer dense_1: 2,570 params, dtype=float32, policy=<Policy "mixed_float16">, size=10.04 KB
   Model File Size: 6.30 MB
   Theoretical Memory: 1.24 MB
   Training Memory: 5.70 MB
   Inference Memory: 1.25 MB
   Memory footprint: 6.95 MB

‚ö° PERFORMANCE & SPEED ANALYSIS
--------------------------------------------------
   Test Accuracy:82.11%
   Test Loss: 0.5594
   Single Sample Time: 3.02 ¬± 21.06 ms
   GPU Available: Yes
   GPU Memory Growth: True
Distributed model compiled with OneDeviceStrategy
Distributed model compiled with MirroredStrategy
OneDevice training time: 147.41787886619568 seconds
Distributed training time: 181.30763244628906 seconds
scaling efficiency: 0.8130814840895738
Starting gradient accumulation batch processing benchmark...
=== GRADIENT ACCUMULATION THROUGHPUT ANALYSIS ===
Testing throughput for batch size: 64
  Accumulation steps: 1
  Results: 1875.61 samples/sec, 29.31 batches/sec
Testing throughput for batch size: 128
  Accumulation steps: 2
  Results: 2689.33 samples/sec, 21.01 batches/sec
Testing throughput for batch size: 256
  Accumulation steps: 4
  Results: 2527.47 samples/sec, 9.87 batches/sec
Testing throughput for batch size: 512
  Accumulation steps: 8
  Results: 2972.96 samples/sec, 5.81 batches/sec
Testing throughput for batch size: 1024
  Accumulation steps: 16
  Results: 2818.09 samples/sec, 2.75 batches/sec
Optimal batch size for throughput: 512
Best throughput: 2972.96 samples/sec
Testing throughput for batch size: 512
  Accumulation steps: 8
  Results: 2615.57 samples/sec, 5.11 batches/sec
Starting knowledge distillation training...
Batch 0/781 - Loss: 6.1384 - Acc: 0.0781
Batch 100/781 - Loss: 5.7196 - Acc: 0.2065
Batch 200/781 - Loss: 5.6691 - Acc: 0.2494
Batch 300/781 - Loss: 5.6128 - Acc: 0.2809
Batch 400/781 - Loss: 5.5701 - Acc: 0.3072
Batch 500/781 - Loss: 5.5415 - Acc: 0.3274
Batch 600/781 - Loss: 5.5054 - Acc: 0.3416
Batch 700/781 - Loss: 5.4787 - Acc: 0.3571
Epoch 1 - Train Loss: 5.4624 - Train Acc: 0.3690 - Val Loss: 6.3204 - Val Acc: 0.3826
Batch 0/781 - Loss: 5.3360 - Acc: 0.4375
Batch 100/781 - Loss: 5.2073 - Acc: 0.4861
Batch 200/781 - Loss: 5.2465 - Acc: 0.4892
Batch 300/781 - Loss: 5.2377 - Acc: 0.4912
Batch 400/781 - Loss: 5.2303 - Acc: 0.4990
Batch 500/781 - Loss: 5.2276 - Acc: 0.5031
Batch 600/781 - Loss: 5.2119 - Acc: 0.5054
Batch 700/781 - Loss: 5.2036 - Acc: 0.5091
Epoch 2 - Train Loss: 5.1988 - Train Acc: 0.5132 - Val Loss: 5.8672 - Val Acc: 0.5304
Batch 0/781 - Loss: 5.1161 - Acc: 0.6250
Batch 100/781 - Loss: 5.0775 - Acc: 0.5433
Batch 200/781 - Loss: 5.1143 - Acc: 0.5513
Batch 300/781 - Loss: 5.1098 - Acc: 0.5517
Batch 400/781 - Loss: 5.1086 - Acc: 0.5576
Batch 500/781 - Loss: 5.1100 - Acc: 0.5603
Batch 600/781 - Loss: 5.0986 - Acc: 0.5623
Batch 700/781 - Loss: 5.0940 - Acc: 0.5648
Epoch 3 - Train Loss: 5.0933 - Train Acc: 0.5678 - Val Loss: 5.8693 - Val Acc: 0.5419
Batch 0/781 - Loss: 5.1428 - Acc: 0.4844
Batch 100/781 - Loss: 4.9976 - Acc: 0.5829
Batch 200/781 - Loss: 5.0350 - Acc: 0.5892
Batch 300/781 - Loss: 5.0349 - Acc: 0.5896
Batch 400/781 - Loss: 5.0383 - Acc: 0.5921
Batch 500/781 - Loss: 5.0404 - Acc: 0.5947
Batch 600/781 - Loss: 5.0296 - Acc: 0.5974
Batch 700/781 - Loss: 5.0266 - Acc: 0.5989
Epoch 4 - Train Loss: 5.0258 - Train Acc: 0.6012 - Val Loss: 5.8785 - Val Acc: 0.5533
Batch 0/781 - Loss: 5.0288 - Acc: 0.6406
Batch 100/781 - Loss: 4.9476 - Acc: 0.6180
Batch 200/781 - Loss: 4.9849 - Acc: 0.6204
Batch 300/781 - Loss: 4.9826 - Acc: 0.6212
Batch 400/781 - Loss: 4.9880 - Acc: 0.6208
Batch 500/781 - Loss: 4.9905 - Acc: 0.6225
Batch 600/781 - Loss: 4.9808 - Acc: 0.6232
Batch 700/781 - Loss: 4.9777 - Acc: 0.6258
Epoch 5 - Train Loss: 4.9775 - Train Acc: 0.6281 - Val Loss: 5.7188 - Val Acc: 0.5953
Batch 0/781 - Loss: 4.9648 - Acc: 0.5781
Batch 100/781 - Loss: 4.9010 - Acc: 0.6349
Batch 200/781 - Loss: 4.9423 - Acc: 0.6399
Batch 300/781 - Loss: 4.9410 - Acc: 0.6420
Batch 400/781 - Loss: 4.9465 - Acc: 0.6429
Batch 500/781 - Loss: 4.9482 - Acc: 0.6457
Batch 600/781 - Loss: 4.9398 - Acc: 0.6467
Batch 700/781 - Loss: 4.9378 - Acc: 0.6486
Epoch 6 - Train Loss: 4.9386 - Train Acc: 0.6499 - Val Loss: 5.9932 - Val Acc: 0.5095
Batch 0/781 - Loss: 4.8408 - Acc: 0.6719
Batch 100/781 - Loss: 4.8645 - Acc: 0.6631
Batch 200/781 - Loss: 4.9081 - Acc: 0.6629
Batch 300/781 - Loss: 4.9107 - Acc: 0.6625
Batch 400/781 - Loss: 4.9150 - Acc: 0.6650
Batch 500/781 - Loss: 4.9182 - Acc: 0.6654
Batch 600/781 - Loss: 4.9087 - Acc: 0.6656
Batch 700/781 - Loss: 4.9063 - Acc: 0.6677
Epoch 7 - Train Loss: 4.9072 - Train Acc: 0.6689 - Val Loss: 5.8923 - Val Acc: 0.5655
Batch 0/781 - Loss: 4.8558 - Acc: 0.6719
Batch 100/781 - Loss: 4.8353 - Acc: 0.6802
Batch 200/781 - Loss: 4.8796 - Acc: 0.6765
Batch 300/781 - Loss: 4.8810 - Acc: 0.6762
Batch 400/781 - Loss: 4.8881 - Acc: 0.6768
Batch 500/781 - Loss: 4.8913 - Acc: 0.6801
Batch 600/781 - Loss: 4.8822 - Acc: 0.6800
Batch 700/781 - Loss: 4.8817 - Acc: 0.6811
Epoch 8 - Train Loss: 4.8830 - Train Acc: 0.6819 - Val Loss: 6.2601 - Val Acc: 0.4753
Batch 0/781 - Loss: 4.7690 - Acc: 0.7344
Batch 100/781 - Loss: 4.8144 - Acc: 0.6863
Batch 200/781 - Loss: 4.8594 - Acc: 0.6863
Batch 300/781 - Loss: 4.8601 - Acc: 0.6863
Batch 400/781 - Loss: 4.8646 - Acc: 0.6884
Batch 500/781 - Loss: 4.8667 - Acc: 0.6909
Batch 600/781 - Loss: 4.8585 - Acc: 0.6914
Batch 700/781 - Loss: 4.8573 - Acc: 0.6929
Epoch 9 - Train Loss: 4.8585 - Train Acc: 0.6942 - Val Loss: 5.7482 - Val Acc: 0.6234
Batch 0/781 - Loss: 4.8015 - Acc: 0.7344
Batch 100/781 - Loss: 4.7951 - Acc: 0.7047
Batch 200/781 - Loss: 4.8427 - Acc: 0.7008
Batch 300/781 - Loss: 4.8425 - Acc: 0.7019
Batch 400/781 - Loss: 4.8468 - Acc: 0.7031
Batch 500/781 - Loss: 4.8491 - Acc: 0.7062
Batch 600/781 - Loss: 4.8417 - Acc: 0.7060
Batch 700/781 - Loss: 4.8402 - Acc: 0.7073
Epoch 10 - Train Loss: 4.8414 - Train Acc: 0.7094 - Val Loss: 5.7260 - Val Acc: 0.6250
Batch 0/781 - Loss: 4.7078 - Acc: 0.7969
Batch 100/781 - Loss: 4.7792 - Acc: 0.7155
Batch 200/781 - Loss: 4.8207 - Acc: 0.7115
Batch 300/781 - Loss: 4.8235 - Acc: 0.7108
Batch 400/781 - Loss: 4.8295 - Acc: 0.7120
Batch 500/781 - Loss: 4.8320 - Acc: 0.7145
Batch 600/781 - Loss: 4.8235 - Acc: 0.7159
Batch 700/781 - Loss: 4.8219 - Acc: 0.7170
Epoch 11 - Train Loss: 4.8234 - Train Acc: 0.7185 - Val Loss: 5.9273 - Val Acc: 0.5656
Batch 0/781 - Loss: 4.7081 - Acc: 0.7656
Batch 100/781 - Loss: 4.7658 - Acc: 0.7208
Batch 200/781 - Loss: 4.8105 - Acc: 0.7192
Batch 300/781 - Loss: 4.8104 - Acc: 0.7213
Batch 400/781 - Loss: 4.8171 - Acc: 0.7205
Batch 500/781 - Loss: 4.8203 - Acc: 0.7234
Batch 600/781 - Loss: 4.8116 - Acc: 0.7243
Batch 700/781 - Loss: 4.8102 - Acc: 0.7252
Epoch 12 - Train Loss: 4.8120 - Train Acc: 0.7262 - Val Loss: 5.6285 - Val Acc: 0.6530
Batch 0/781 - Loss: 4.7537 - Acc: 0.8281
Batch 100/781 - Loss: 4.7545 - Acc: 0.7274
Batch 200/781 - Loss: 4.7994 - Acc: 0.7273
Batch 300/781 - Loss: 4.7996 - Acc: 0.7287
Batch 400/781 - Loss: 4.8050 - Acc: 0.7297
Batch 500/781 - Loss: 4.8076 - Acc: 0.7317
Batch 600/781 - Loss: 4.7996 - Acc: 0.7314
Batch 700/781 - Loss: 4.7990 - Acc: 0.7329
Epoch 13 - Train Loss: 4.8010 - Train Acc: 0.7335 - Val Loss: 5.6000 - Val Acc: 0.6711
Batch 0/781 - Loss: 4.7101 - Acc: 0.7812
Batch 100/781 - Loss: 4.7429 - Acc: 0.7396
Batch 200/781 - Loss: 4.7868 - Acc: 0.7369
Batch 300/781 - Loss: 4.7870 - Acc: 0.7375
Batch 400/781 - Loss: 4.7937 - Acc: 0.7367
Batch 500/781 - Loss: 4.7991 - Acc: 0.7374
Batch 600/781 - Loss: 4.7910 - Acc: 0.7375
Batch 700/781 - Loss: 4.7901 - Acc: 0.7386
Epoch 14 - Train Loss: 4.7915 - Train Acc: 0.7401 - Val Loss: 5.7330 - Val Acc: 0.6304
Batch 0/781 - Loss: 4.6707 - Acc: 0.8125
Batch 100/781 - Loss: 4.7327 - Acc: 0.7432
Batch 200/781 - Loss: 4.7799 - Acc: 0.7387
Batch 300/781 - Loss: 4.7779 - Acc: 0.7426
Batch 400/781 - Loss: 4.7849 - Acc: 0.7422
Batch 500/781 - Loss: 4.7876 - Acc: 0.7444
Batch 600/781 - Loss: 4.7794 - Acc: 0.7447
Batch 700/781 - Loss: 4.7784 - Acc: 0.7456
Epoch 15 - Train Loss: 4.7802 - Train Acc: 0.7465 - Val Loss: 5.6485 - Val Acc: 0.6509
Batch 0/781 - Loss: 4.7365 - Acc: 0.7500
Batch 100/781 - Loss: 4.7235 - Acc: 0.7477
Batch 200/781 - Loss: 4.7707 - Acc: 0.7472
Batch 300/781 - Loss: 4.7698 - Acc: 0.7468
Batch 400/781 - Loss: 4.7767 - Acc: 0.7473
Batch 500/781 - Loss: 4.7812 - Acc: 0.7492
Batch 600/781 - Loss: 4.7724 - Acc: 0.7491
Batch 700/781 - Loss: 4.7717 - Acc: 0.7504
Epoch 16 - Train Loss: 4.7737 - Train Acc: 0.7517 - Val Loss: 5.6732 - Val Acc: 0.6467
Batch 0/781 - Loss: 4.7039 - Acc: 0.7656
Batch 100/781 - Loss: 4.7179 - Acc: 0.7536
Batch 200/781 - Loss: 4.7617 - Acc: 0.7525
Batch 300/781 - Loss: 4.7609 - Acc: 0.7548
Batch 400/781 - Loss: 4.7689 - Acc: 0.7540
Batch 500/781 - Loss: 4.7734 - Acc: 0.7549
Batch 600/781 - Loss: 4.7654 - Acc: 0.7551
Batch 700/781 - Loss: 4.7645 - Acc: 0.7565
Epoch 17 - Train Loss: 4.7656 - Train Acc: 0.7581 - Val Loss: 5.7264 - Val Acc: 0.6314
Batch 0/781 - Loss: 4.6965 - Acc: 0.7969
Batch 100/781 - Loss: 4.7094 - Acc: 0.7602
Batch 200/781 - Loss: 4.7547 - Acc: 0.7607
Batch 300/781 - Loss: 4.7529 - Acc: 0.7617
Batch 400/781 - Loss: 4.7600 - Acc: 0.7594
Batch 500/781 - Loss: 4.7635 - Acc: 0.7611
Batch 600/781 - Loss: 4.7560 - Acc: 0.7601
Batch 700/781 - Loss: 4.7546 - Acc: 0.7614
Epoch 18 - Train Loss: 4.7571 - Train Acc: 0.7623 - Val Loss: 5.6168 - Val Acc: 0.6733
Batch 0/781 - Loss: 4.6692 - Acc: 0.8594
Batch 100/781 - Loss: 4.6959 - Acc: 0.7649
Batch 200/781 - Loss: 4.7439 - Acc: 0.7620
Batch 300/781 - Loss: 4.7450 - Acc: 0.7615
Batch 400/781 - Loss: 4.7521 - Acc: 0.7626
Batch 500/781 - Loss: 4.7561 - Acc: 0.7639
Batch 600/781 - Loss: 4.7481 - Acc: 0.7638
Batch 700/781 - Loss: 4.7470 - Acc: 0.7658
Epoch 19 - Train Loss: 4.7497 - Train Acc: 0.7663 - Val Loss: 5.5785 - Val Acc: 0.6831
Batch 0/781 - Loss: 4.6981 - Acc: 0.8281
Batch 100/781 - Loss: 4.6880 - Acc: 0.7718
Batch 200/781 - Loss: 4.7379 - Acc: 0.7711
Batch 300/781 - Loss: 4.7378 - Acc: 0.7698
Batch 400/781 - Loss: 4.7448 - Acc: 0.7689
Batch 500/781 - Loss: 4.7508 - Acc: 0.7691
Batch 600/781 - Loss: 4.7428 - Acc: 0.7690
Batch 700/781 - Loss: 4.7413 - Acc: 0.7702
Epoch 20 - Train Loss: 4.7433 - Train Acc: 0.7713 - Val Loss: 5.7597 - Val Acc: 0.6307
Knowledge distillation training completed!

============================================================
üîç STREAMLINED ANALYSIS: part2_cloud_optimization/student_model.keras
============================================================

üèóÔ∏è  ARCHITECTURE & MEMORY ANALYSIS
--------------------------------------------------
   Total Parameters: 82,586
   Trainable Parameters: 82,138
   Layer conv2d: 448 params, dtype=float32, policy=<Policy "float32">, size=1.75 KB
   Layer batch_normalization: 64 params, dtype=float32, policy=<Policy "float32">, size=0.25 KB
   Layer conv2d_1: 2,320 params, dtype=float32, policy=<Policy "float32">, size=9.06 KB
   Layer batch_normalization_1: 64 params, dtype=float32, policy=<Policy "float32">, size=0.25 KB
   Layer conv2d_2: 4,640 params, dtype=float32, policy=<Policy "float32">, size=18.12 KB
   Layer batch_normalization_2: 128 params, dtype=float32, policy=<Policy "float32">, size=0.50 KB
   Layer conv2d_3: 9,248 params, dtype=float32, policy=<Policy "float32">, size=36.12 KB
   Layer batch_normalization_3: 128 params, dtype=float32, policy=<Policy "float32">, size=0.50 KB
   Layer conv2d_4: 18,496 params, dtype=float32, policy=<Policy "float32">, size=72.25 KB
   Layer batch_normalization_4: 256 params, dtype=float32, policy=<Policy "float32">, size=1.00 KB
   Layer conv2d_5: 36,928 params, dtype=float32, policy=<Policy "float32">, size=144.25 KB
   Layer batch_normalization_5: 256 params, dtype=float32, policy=<Policy "float32">, size=1.00 KB
   Layer dense: 8,320 params, dtype=float32, policy=<Policy "float32">, size=32.50 KB
   Layer dense_1: 1,290 params, dtype=float32, policy=<Policy "float32">, size=5.04 KB
   Model File Size: 0.39 MB
   Theoretical Memory: 0.32 MB
   Training Memory: 2.01 MB
   Inference Memory: 0.33 MB
   Memory footprint: 2.34 MB

‚ö° PERFORMANCE & SPEED ANALYSIS
--------------------------------------------------
   Test Accuracy:63.08%
   Test Loss: 1.7087
   Single Sample Time: 3.01 ¬± 24.87 ms
   GPU Available: Yes
   GPU Memory Growth: True

============================================================
üîç STREAMLINED ANALYSIS: part2_cloud_optimization/teacher_model.keras
============================================================

üèóÔ∏è  ARCHITECTURE & MEMORY ANALYSIS
--------------------------------------------------
   Total Parameters: 324,394
   Trainable Parameters: 323,498
   Layer conv2d: 896 params, dtype=float32, policy=<Policy "float32">, size=3.50 KB
   Layer batch_normalization: 128 params, dtype=float32, policy=<Policy "float32">, size=0.50 KB
   Layer conv2d_1: 9,248 params, dtype=float32, policy=<Policy "float32">, size=36.12 KB
   Layer batch_normalization_1: 128 params, dtype=float32, policy=<Policy "float32">, size=0.50 KB
   Layer conv2d_2: 18,496 params, dtype=float32, policy=<Policy "float32">, size=72.25 KB
   Layer batch_normalization_2: 256 params, dtype=float32, policy=<Policy "float32">, size=1.00 KB
   Layer conv2d_3: 36,928 params, dtype=float32, policy=<Policy "float32">, size=144.25 KB
   Layer batch_normalization_3: 256 params, dtype=float32, policy=<Policy "float32">, size=1.00 KB
   Layer conv2d_4: 73,856 params, dtype=float32, policy=<Policy "float32">, size=288.50 KB
   Layer batch_normalization_4: 512 params, dtype=float32, policy=<Policy "float32">, size=2.00 KB
   Layer conv2d_5: 147,584 params, dtype=float32, policy=<Policy "float32">, size=576.50 KB
   Layer batch_normalization_5: 512 params, dtype=float32, policy=<Policy "float32">, size=2.00 KB
   Layer dense: 33,024 params, dtype=float32, policy=<Policy "float32">, size=129.00 KB
   Layer dense_1: 2,570 params, dtype=float32, policy=<Policy "float32">, size=10.04 KB
   Model File Size: 1.32 MB
   Theoretical Memory: 1.24 MB
   Training Memory: 5.70 MB
   Inference Memory: 1.25 MB
   Memory footprint: 6.95 MB

‚ö° PERFORMANCE & SPEED ANALYSIS
--------------------------------------------------
   Test Accuracy:78.31%
   Test Loss: 1.5131
   Single Sample Time: 1.37 ¬± 3.55 ms
   GPU Available: Yes
   GPU Memory Growth: True
